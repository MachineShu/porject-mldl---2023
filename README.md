Коды：
MNIST：
[https://www.kaggle.com/code/machinetree/mnist-research](https://www.kaggle.com/code/machinetree/mnist-research)
Omniglot：
[https://www.kaggle.com/code/machinetree/omniglot-research](https://www.kaggle.com/code/machinetree/omniglot-research)
Cifar-10:
[https://www.kaggle.com/code/machinetree/cifar-10-research](https://www.kaggle.com/code/machinetree/cifar-10-research)

# Анализ производительности байесовских нейронных сетей (BNN) с различными априорными весами на различных наборах данных (MNIST, Omniglot, CIFAR-10)

## Аннотация:

В данной статье предложена и проанализирована BNN с использованием различных априорных весов (равномерных, гауссовых, лапласовых) для распознавания изображений на различных наборах данных (MNIST, Omniglot, CIFAR-10). Модель обучалась методами вариационного вывода (VI) и марковской цепи Монте-Карло (MCMC). Результаты экспериментов показали, что различные априорные веса и методы обучения демонстрируют разную производительность на различных наборах данных.
## 1.Контекст исследования

С развитием глубокого обучения нейронные сети стали широко применяться в различных областях, например компьютерном зрении или обработке естественного языка. Однако традиционные нейронные сети имеют проблемы - переобучение и неопределенность модели. Для их решения была разработана байесовская нейронная сеть (BNN).

В BNN веса уже не являются фиксированными значениями, а становятся случайными переменными, а постериорное распределение вероятности вычисляется на основе байесовской формулы, что позволяет лучше оценивать неопределенность модели. Так как веса становятся случайными переменными, необходимо назначить им априорное распределение вероятности, а различные априорные распределения могут влиять на производительность модели. Поэтому данная статья исследует производительность BNN с использованием различных априорных весов (равномерных, гауссовых, лапласовых) на различных наборах данных (MNIST, Omniglot, CIFAR-10), а также сравнивает методы вариационного вывода и марковской цепи Монте-Карло.

## 2.Связанные исследования

Байесовские нейронные сети - это продукт сочетания нейронных сетей и байесовского подхода, и много исследований исследовали влияние различных априорных весов и методов обучения на производительность BNN. Например, исследования показали, что использование гауссовского априорного веса на наборе данных MNIST дает хороший результат, а использование априорного веса гауссовского на наборе данных CIFAR-10 дает лучший результат. Кроме того, некоторые исследования изучили влияние различных методов обучения на производительность BNN, таких как методы вариационного вывода и методы Марковской цепи Монте-Карло. В данной работе на основе существующих исследований была проверена эффективность различных априорных весов и методов обучения на разных наборах данных.

## 3.Проектирование модели

В данной работе мы используем стандартную сверточную нейронную сеть в качестве базовой модели и заменяем веса на байесовские веса. Для каждого веса мы используем равномерное распределение, гауссовское распределение и распределение Лапласа в качестве априорного. В процессе реализации мы используем фреймворк PyTorch, который является очень популярным фреймворком для глубокого обучения.

## 4.Датасет и предварительная обработка

### 4.1 Описание датасета

В данной работе мы использовали три общедоступных датасета: MNIST, Omniglot и CIFAR-10. Эти датасеты широко применяются в области классификации изображений и имеют различные характеристики в терминах уровня сложности и количества образцов. Ниже кратко описывается каждый из этих датасетов.

MNIST: MNIST - это датасет изображений рукописных цифр, содержащий 60 000 образцов для обучения и 10 000 образцов для тестирования. Размер каждого изображения составляет 28x28 пикселей. MNIST широко используется для тестирования производительности моделей глубокого обучения и считается относительно простым датасетом.

Omniglot: Omniglot - это датасет малых образцов, содержащий 1623 рукописных символа из 50 различных языков. В датасете каждый символ имеет 20 образцов, общее число изображений составляет 38 560. Omniglot - это датасет с вызовами, используемый для тестирования обобщающей способности моделей при работе с малым количеством образцов.

CIFAR-10: CIFAR-10 - это датасет классификации изображений, содержащий 100 категорий, каждая из которых состоит из 600 цветных изображений размером 32x32 пикселей. В датасете 50 000 изображений используются для обучения и 10 000 для тестирования. CIFAR-10 является более сложным датасетом, чем MNIST и Omniglot, с более высоким разрешением изображений и большим количеством категорий.

### 4.2 Дополнительная обработка данных

Для повышения обобщающей способности модели мы произвели предварительную обработку данных. Конкретно, мы выполнили следующие операции:

Масштабирование: мы масштабировали все изображения до размера $$32\times32$$ пикселей, чтобы соответствовать размеру входного слоя модели.

Увеличение данных: мы использовали техники увеличения данных для расширения набора данных. Конкретно, мы случайно поворачивали, перемещали и переворачивали обучающую выборку для увеличения разнообразия примеров.

С помощью вышеуказанных операций предварительной обработки мы получили набор данных, подходящий для обучения BNN.

## 5.Тренировочная стратегия

В данном эксперименте мы использовали два различных метода обучения: метод вариационного вывода (VI) и метод Монте-Карло по цепям Маркова (MCMC).

Метод вариационного вывода (VI) - это метод приближенного вывода, который может быстро оптимизировать параметры модели. Этот метод предполагает, что апостериорное распределение имеет какую-то форму распределения, такую как гауссово распределение или распределение экспоненциального семейства. Затем метод VI приближает истинное апостериорное распределение путем поиска близкого распределения. Это приближенное распределение может быть получено путем оптимизации параметров.

Метод MCMC является методом на основе Монте-Карло-сэмплирования, который может более точно оценивать апостериорное распределение. Этот метод приближает истинное апостериорное распределение путем генерации набора образцов, которые обычно генерируются путем случайного блуждания. По сравнению с методом VI, метод MCMC имеет более высокую вычислительную сложность, но обычно может получать более точные результаты.

Мы обучали модели с использованием методов VI и MCMC на разных наборах данных и с различными априорными весами, и сравнивали их производительность. Конкретно, мы использовали реализацию PyTorch и установили соответствующие гиперпараметры и количество эпох в эксперименте, чтобы убедиться, что модель сходится достаточно хорошо.

## 6.Результаты эксперимента

Мы сравнили результаты экспериментов с использованием разных предварительных весов и методов обучения на трех наборах данных. Результаты эксперимента показали, что различные методы обучения имеют свои преимущества в зависимости от набора данных и предварительных весов.

На наборах данных MNIST и CIFAR-10 предварительные гауссовые веса показали лучшие результаты как для метода вариационного вывода. 

Следует отметить, что вычислительная сложность метода MCMC выше, чем у метода VI, поэтому в случае ограниченных ресурсов метод VI может быть лучшим выбором. Кроме того, мы также сравнили влияние различных априорных распределений на производительность BNN. 

На наборах данных MNIST и CIFAR-10 гауссовское априорное распределение показало хорошие результаты при использовании метода VI. Это говорит о том, что различные априорные распределения могут быть более или менее подходящими в зависимости от конкретного набора данных.

Формы:
https://github.com/MachineShu/porject-mldl---2023/blob/main/1.png
https://github.com/MachineShu/porject-mldl---2023/blob/main/2.png

Очевидно, что равномерное распределение дает лучшие результаты, модель не смещена в сторону какого-либо конкретного значения и более эффективно использует свои параметры.

## 7.Заключение

Результаты нашего эксперимента подтверждают, что Байесовские нейронные сети как новый тип модели нейронных сетей обладают высокой производительностью и способностью оценки неопределенности.

Например, гауссовское предшествование обычно имеет тенденцию выбирать меньшие веса, что помогает предотвратить чрезмерную подгонку, но может также ограничить возможности модели. Равномерное распределение может дать модели больше возможностей, так как оно считается одинаково вероятным для всех значений в диапазоне. В частности, для сложных моделей выбор предварительного распределения влияет на конечный эффект модели.

Равномерное распределение может дать модели больше возможностей, поскольку все значения в диапазоне считаются одинаково вероятными. Это может помочь модели найти лучшие решения, особенно при наличии нескольких локально оптимальных решений.

MCMC работает менее эффективно, чем VI, либо потому, что модель недостаточно сложна, либо потому, что не хватает вычислительных ресурсов, чтобы вызвать недооценку. VI обычно дает детерминированный результат, тогда как MCMC дает распределение параметров, что может сделать результаты более сложными для интерпретации.
