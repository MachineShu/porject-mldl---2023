{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Omniglot","metadata":{}},{"cell_type":"markdown","source":"## 1.Gaussian weights","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision.datasets import Omniglot\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:39:54.366041Z","iopub.execute_input":"2023-05-10T03:39:54.366408Z","iopub.status.idle":"2023-05-10T03:39:58.143165Z","shell.execute_reply.started":"2023-05-10T03:39:54.366381Z","shell.execute_reply":"2023-05-10T03:39:58.142069Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load Omniglot dataset\ntransform = transforms.Compose([transforms.Resize((28, 28)),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5,), (0.5,))])\ntrain_dataset = Omniglot(root=\"./data\", download=True, transform=transform)\ntest_dataset = Omniglot(root=\"./data\", download=True, transform=transform)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:39:58.145317Z","iopub.execute_input":"2023-05-10T03:39:58.146024Z","iopub.status.idle":"2023-05-10T03:40:02.276489Z","shell.execute_reply.started":"2023-05-10T03:39:58.145983Z","shell.execute_reply":"2023-05-10T03:40:02.275348Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_background.zip to ./data/omniglot-py/images_background.zip\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9464212/9464212 [00:00<00:00, 179087334.65it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/omniglot-py/images_background.zip to ./data/omniglot-py\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Files already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### VI","metadata":{}},{"cell_type":"code","source":"# Model with Gaussian prior \nclass BNN(nn.Module):\n      def __init__(self, n_in, n_out,n_hide):\n        super().__init__()\n        self.w = nn.Parameter(torch.randn(n_in, n_out))\n        self.fc1 = nn.Linear(n_in, n_hide) \n        self.fc2 = nn.Linear(n_hide, 964)\n        self.dropout = nn.Dropout(0.5) \n    \n      def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)  \n  \n      def kl_divergence(self): \n        mean, std = self.w.mean(), self.w.std()\n        return ((mean ** 2 + std ** 2 - torch.log(std **2)-1)/2).sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:40:02.278308Z","iopub.execute_input":"2023-05-10T03:40:02.278650Z","iopub.status.idle":"2023-05-10T03:40:02.288286Z","shell.execute_reply.started":"2023-05-10T03:40:02.278622Z","shell.execute_reply":"2023-05-10T03:40:02.287390Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"model = BNN(784, 400, 964)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:40:02.289441Z","iopub.execute_input":"2023-05-10T03:40:02.290313Z","iopub.status.idle":"2023-05-10T03:40:02.371912Z","shell.execute_reply.started":"2023-05-10T03:40:02.290281Z","shell.execute_reply":"2023-05-10T03:40:02.370791Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nfor epoch in range(50):\n    for x, y in train_loader:\n        x = x.view(-1, 784)\n        loss = F.nll_loss(model(x), y)\n        loss += model.kl_divergence() * 0.1   #添加KL散度\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'Epoch: {epoch+1}, Train Loss: {loss.item():.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:40:04.316476Z","iopub.execute_input":"2023-05-10T03:40:04.316869Z","iopub.status.idle":"2023-05-10T03:53:21.502575Z","shell.execute_reply.started":"2023-05-10T03:40:04.316840Z","shell.execute_reply":"2023-05-10T03:53:21.501427Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch: 1, Train Loss: 6.866\nEpoch: 2, Train Loss: 6.843\nEpoch: 3, Train Loss: 6.742\nEpoch: 4, Train Loss: 6.665\nEpoch: 5, Train Loss: 6.639\nEpoch: 6, Train Loss: 6.562\nEpoch: 7, Train Loss: 6.403\nEpoch: 8, Train Loss: 6.547\nEpoch: 9, Train Loss: 6.433\nEpoch: 10, Train Loss: 6.417\nEpoch: 11, Train Loss: 6.493\nEpoch: 12, Train Loss: 6.566\nEpoch: 13, Train Loss: 6.400\nEpoch: 14, Train Loss: 6.507\nEpoch: 15, Train Loss: 6.554\nEpoch: 16, Train Loss: 6.411\nEpoch: 17, Train Loss: 6.398\nEpoch: 18, Train Loss: 6.534\nEpoch: 19, Train Loss: 6.349\nEpoch: 20, Train Loss: 6.468\nEpoch: 21, Train Loss: 6.275\nEpoch: 22, Train Loss: 6.400\nEpoch: 23, Train Loss: 6.287\nEpoch: 24, Train Loss: 6.455\nEpoch: 25, Train Loss: 6.369\nEpoch: 26, Train Loss: 6.195\nEpoch: 27, Train Loss: 6.382\nEpoch: 28, Train Loss: 6.357\nEpoch: 29, Train Loss: 6.341\nEpoch: 30, Train Loss: 6.421\nEpoch: 31, Train Loss: 6.248\nEpoch: 32, Train Loss: 6.390\nEpoch: 33, Train Loss: 6.198\nEpoch: 34, Train Loss: 6.422\nEpoch: 35, Train Loss: 6.357\nEpoch: 36, Train Loss: 6.214\nEpoch: 37, Train Loss: 6.423\nEpoch: 38, Train Loss: 6.421\nEpoch: 39, Train Loss: 6.165\nEpoch: 40, Train Loss: 6.344\nEpoch: 41, Train Loss: 6.208\nEpoch: 42, Train Loss: 6.357\nEpoch: 43, Train Loss: 6.217\nEpoch: 44, Train Loss: 6.160\nEpoch: 45, Train Loss: 6.331\nEpoch: 46, Train Loss: 6.133\nEpoch: 47, Train Loss: 6.280\nEpoch: 48, Train Loss: 6.260\nEpoch: 49, Train Loss: 6.190\nEpoch: 50, Train Loss: 6.420\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test accuracy\nwith torch.no_grad():\n    correct = 0\n    for x, y in test_loader:\n        x = x.view(-1, 784)\n        y_pred = model(x).argmax(dim=1)\n        correct += (y_pred == y).sum().item()\n\nprint(f'Test accuracy: {correct/len(test_dataset):.5f}')","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:53:46.570190Z","iopub.execute_input":"2023-05-10T03:53:46.570576Z","iopub.status.idle":"2023-05-10T03:53:57.877532Z","shell.execute_reply.started":"2023-05-10T03:53:46.570549Z","shell.execute_reply":"2023-05-10T03:53:57.876164Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Test accuracy: 0.40716\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 1.2MCMC","metadata":{}},{"cell_type":"code","source":"# Get train data\ntrain_x = []  \ntrain_y = []\nfor x, y in train_loader:\n    train_x.append(x.view(-1, 784))\n    train_y.append(y)\ntrain_x = torch.cat(train_x)  \ntrain_y = torch.cat(train_y)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:54:01.665231Z","iopub.execute_input":"2023-05-10T03:54:01.666106Z","iopub.status.idle":"2023-05-10T03:54:12.312601Z","shell.execute_reply.started":"2023-05-10T03:54:01.666067Z","shell.execute_reply":"2023-05-10T03:54:12.311604Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# MCMC Sampling\nmodel = BNN(784, 964, 400)\nw = model.w.data  # Get weight tensor ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:54:12.314151Z","iopub.execute_input":"2023-05-10T03:54:12.315662Z","iopub.status.idle":"2023-05-10T03:54:12.332635Z","shell.execute_reply.started":"2023-05-10T03:54:12.315631Z","shell.execute_reply":"2023-05-10T03:54:12.331800Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for i in range(1500):  # 1500 iterations\n    # Proposal distribution\n    w_proposal = w + torch.randn(w.size())*0.1 \n\n    # Acceptance ratio\n    outputs = model(train_x)  # log_softmax输出\n    ap = torch.exp(outputs.gather(1, train_y.unsqueeze(1)).sum()) \n\n    # Metropolis acceptance\n    u = torch.rand(1)\n    if u < ap:\n        w = w_proposal  # Accept proposal \n\n    model.w.data = w  # Set weight to sampled value","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:54:13.960313Z","iopub.execute_input":"2023-05-10T03:54:13.960930Z","iopub.status.idle":"2023-05-10T04:02:23.909492Z","shell.execute_reply.started":"2023-05-10T03:54:13.960895Z","shell.execute_reply":"2023-05-10T04:02:23.908383Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Test accuracy\nwith torch.no_grad():\n    correct = 0\n    for x, y in test_loader:\n        x = x.view(-1, 784)\n        y_pred = model(x).argmax(dim=1) \n        correct += (y_pred == y).sum().item()\n    print(f'Test accuracy: {correct/len(test_dataset):.5f}') ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:06:27.119673Z","iopub.execute_input":"2023-05-10T04:06:27.120086Z","iopub.status.idle":"2023-05-10T04:06:38.380015Z","shell.execute_reply.started":"2023-05-10T04:06:27.120057Z","shell.execute_reply":"2023-05-10T04:06:38.378813Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Test accuracy: 0.10270\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2.Laplace weights","metadata":{}},{"cell_type":"markdown","source":"### 2.1Variable Inference","metadata":{}},{"cell_type":"code","source":"# Model with Laplace prior \nclass BNN(nn.Module):\n    def __init__(self, n_in, n_out,n_hide):\n        super().__init__()\n        self.w = nn.Parameter(torch.zeros(n_in, n_out))\n        self.fc1 = nn.Linear(n_in, n_hide)  \n        self.fc2 = nn.Linear(n_hide, 964)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)  \n\n    def kl_divergence(self):\n        return (self.w.abs().sum() / 2).sum()  # Laplace prior  ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:41:27.676920Z","iopub.execute_input":"2023-05-10T04:41:27.677466Z","iopub.status.idle":"2023-05-10T04:41:27.689101Z","shell.execute_reply.started":"2023-05-10T04:41:27.677427Z","shell.execute_reply":"2023-05-10T04:41:27.687432Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"model = BNN(784, 400, 964) ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:41:30.309189Z","iopub.execute_input":"2023-05-10T04:41:30.309573Z","iopub.status.idle":"2023-05-10T04:41:30.332226Z","shell.execute_reply.started":"2023-05-10T04:41:30.309539Z","shell.execute_reply":"2023-05-10T04:41:30.331262Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nfor epoch in range(50):\n    for x, y in train_loader:\n        x = x.view(-1, 784)\n        loss = F.nll_loss(model(x), y)\n        loss += model.kl_divergence() * 0.1   \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'Epoch: {epoch+1}, Train Loss: {loss.item():.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:41:35.263180Z","iopub.execute_input":"2023-05-10T04:41:35.263575Z","iopub.status.idle":"2023-05-10T04:54:17.379665Z","shell.execute_reply.started":"2023-05-10T04:41:35.263544Z","shell.execute_reply":"2023-05-10T04:54:17.378361Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Epoch: 1, Train Loss: 6.878\nEpoch: 2, Train Loss: 6.821\nEpoch: 3, Train Loss: 6.807\nEpoch: 4, Train Loss: 6.828\nEpoch: 5, Train Loss: 6.752\nEpoch: 6, Train Loss: 6.799\nEpoch: 7, Train Loss: 6.745\nEpoch: 8, Train Loss: 6.727\nEpoch: 9, Train Loss: 6.727\nEpoch: 10, Train Loss: 6.764\nEpoch: 11, Train Loss: 6.695\nEpoch: 12, Train Loss: 6.603\nEpoch: 13, Train Loss: 6.738\nEpoch: 14, Train Loss: 6.570\nEpoch: 15, Train Loss: 6.450\nEpoch: 16, Train Loss: 6.662\nEpoch: 17, Train Loss: 6.550\nEpoch: 18, Train Loss: 6.570\nEpoch: 19, Train Loss: 6.544\nEpoch: 20, Train Loss: 6.505\nEpoch: 21, Train Loss: 6.434\nEpoch: 22, Train Loss: 6.385\nEpoch: 23, Train Loss: 6.295\nEpoch: 24, Train Loss: 6.285\nEpoch: 25, Train Loss: 6.450\nEpoch: 26, Train Loss: 6.310\nEpoch: 27, Train Loss: 6.313\nEpoch: 28, Train Loss: 6.379\nEpoch: 29, Train Loss: 6.403\nEpoch: 30, Train Loss: 6.272\nEpoch: 31, Train Loss: 6.232\nEpoch: 32, Train Loss: 6.224\nEpoch: 33, Train Loss: 6.370\nEpoch: 34, Train Loss: 6.313\nEpoch: 35, Train Loss: 6.213\nEpoch: 36, Train Loss: 6.303\nEpoch: 37, Train Loss: 6.251\nEpoch: 38, Train Loss: 6.253\nEpoch: 39, Train Loss: 6.237\nEpoch: 40, Train Loss: 6.468\nEpoch: 41, Train Loss: 6.322\nEpoch: 42, Train Loss: 6.385\nEpoch: 43, Train Loss: 6.293\nEpoch: 44, Train Loss: 6.404\nEpoch: 45, Train Loss: 6.279\nEpoch: 46, Train Loss: 6.369\nEpoch: 47, Train Loss: 6.306\nEpoch: 48, Train Loss: 6.250\nEpoch: 49, Train Loss: 6.178\nEpoch: 50, Train Loss: 6.272\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test accuracy  \nwith torch.no_grad():\n    correct = 0\n    for x, y in test_loader:\n        x = x.view(-1, 784)\n        y_pred = model(x).argmax(dim=1)\n        correct += (y_pred == y).sum().item() \nprint(f'Test accuracy: {correct/len(test_dataset):.5f}') ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:54:42.938441Z","iopub.execute_input":"2023-05-10T04:54:42.939611Z","iopub.status.idle":"2023-05-10T04:54:54.462604Z","shell.execute_reply.started":"2023-05-10T04:54:42.939552Z","shell.execute_reply":"2023-05-10T04:54:54.461472Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Test accuracy: 0.42116\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 2.2MCMC","metadata":{}},{"cell_type":"code","source":"# Get train data\ntrain_x = []  \ntrain_y = []\nfor x, y in train_loader:\n    train_x.append(x.view(-1, 784))\n    train_y.append(y)\ntrain_x = torch.cat(train_x)  \ntrain_y = torch.cat(train_y)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:19:44.971843Z","iopub.execute_input":"2023-05-10T04:19:44.973059Z","iopub.status.idle":"2023-05-10T04:19:55.242110Z","shell.execute_reply.started":"2023-05-10T04:19:44.973027Z","shell.execute_reply":"2023-05-10T04:19:55.241084Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Model with Laplace prior\nclass BNN(nn.Module):\n    def __init__(self, n_in, n_hide, n_out):\n        super().__init__()\n        self.fc1 = nn.Linear(n_in, n_hide)  \n        self.fc2 = nn.Linear(n_hide, n_out)\n        self.dropout = nn.Dropout(0.5)\n        self.w = torch.zeros(n_in, n_out)  \n\n    def forward(self, x, w_proposal=None):\n        if w_proposal is None:  \n              w = self.w\n        else:\n              w = w_proposal\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        pred = F.log_softmax(x, dim=1)\n        dist = torch.distributions.Categorical(logits=pred)\n        return dist\n\n    def log_prior(self, w):\n        return -w.abs().sum() ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:19:55.245248Z","iopub.execute_input":"2023-05-10T04:19:55.245683Z","iopub.status.idle":"2023-05-10T04:19:55.256243Z","shell.execute_reply.started":"2023-05-10T04:19:55.245644Z","shell.execute_reply":"2023-05-10T04:19:55.254816Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# MCMC Sampling\nmodel = BNN(784, 964, 400) \nw = model.w.data  # Get weight tensor ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:19:55.258189Z","iopub.execute_input":"2023-05-10T04:19:55.258596Z","iopub.status.idle":"2023-05-10T04:19:55.281481Z","shell.execute_reply.started":"2023-05-10T04:19:55.258561Z","shell.execute_reply":"2023-05-10T04:19:55.280585Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# training\nfor i in range(1500): \n    # Proposal distribution\n    w_proposal = w + torch.randn(w.size())*0.1\n\n    # Prior ratio \n    lp = model.log_prior(w_proposal)  \n    ap = torch.exp(lp - model.log_prior(w))\n\n    # Metropolis acceptance \n    u = torch.rand(1)\n    if u < ap:\n        w = w_proposal  \n\n    model.w.data = w ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:19:55.282751Z","iopub.execute_input":"2023-05-10T04:19:55.283612Z","iopub.status.idle":"2023-05-10T04:19:59.757425Z","shell.execute_reply.started":"2023-05-10T04:19:55.283574Z","shell.execute_reply":"2023-05-10T04:19:59.756383Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Test accuracy\nwith torch.no_grad():\n    correct = 0\n    for x, y in test_loader:\n        x = x.view(-1, 784)\n        y_pred = model(x).sample()\n        correct += (y_pred == y).sum().item()\n    print(f'Test accuracy: {correct/len(test_dataset):.5f}') ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:19:59.758642Z","iopub.execute_input":"2023-05-10T04:19:59.759230Z","iopub.status.idle":"2023-05-10T04:20:10.697800Z","shell.execute_reply.started":"2023-05-10T04:19:59.759200Z","shell.execute_reply":"2023-05-10T04:20:10.696479Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Test accuracy: 0.09803\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3.Equalisation weights","metadata":{}},{"cell_type":"markdown","source":"### 3.1Variable Inference","metadata":{}},{"cell_type":"code","source":"# Model with Uniform prior\nclass BNN(nn.Module):\n    def __init__(self, n_in, n_out,n_hide):\n        super().__init__()\n        self.w = nn.Parameter(torch.zeros(n_in, n_out))\n        self.fc1 = nn.Linear(n_in, n_hide)\n        self.fc2 = nn.Linear(n_hide, 964)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)  \n\n    def kl_divergence(self):\n        return 0.5*(self.w**2).sum()  # Uniform prior  ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:20:10.699593Z","iopub.execute_input":"2023-05-10T04:20:10.700515Z","iopub.status.idle":"2023-05-10T04:20:10.710139Z","shell.execute_reply.started":"2023-05-10T04:20:10.700472Z","shell.execute_reply":"2023-05-10T04:20:10.709013Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model = BNN(784, 400, 964) ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:20:10.711898Z","iopub.execute_input":"2023-05-10T04:20:10.712329Z","iopub.status.idle":"2023-05-10T04:20:10.736486Z","shell.execute_reply.started":"2023-05-10T04:20:10.712293Z","shell.execute_reply":"2023-05-10T04:20:10.735458Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nfor epoch in range(50):\n    for x, y in train_loader:\n        x = x.view(-1, 784)\n        loss = F.nll_loss(model(x), y)\n        loss += model.kl_divergence() * 0.1   \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'Epoch: {epoch+1}, Train Loss: {loss.item():.3f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:20:10.740825Z","iopub.execute_input":"2023-05-10T04:20:10.741272Z","iopub.status.idle":"2023-05-10T04:32:49.258936Z","shell.execute_reply.started":"2023-05-10T04:20:10.741232Z","shell.execute_reply":"2023-05-10T04:32:49.257912Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch: 1, Train Loss: 6.856\nEpoch: 2, Train Loss: 6.737\nEpoch: 3, Train Loss: 6.583\nEpoch: 4, Train Loss: 6.467\nEpoch: 5, Train Loss: 6.348\nEpoch: 6, Train Loss: 6.424\nEpoch: 7, Train Loss: 6.168\nEpoch: 8, Train Loss: 6.203\nEpoch: 9, Train Loss: 6.385\nEpoch: 10, Train Loss: 6.391\nEpoch: 11, Train Loss: 6.072\nEpoch: 12, Train Loss: 6.200\nEpoch: 13, Train Loss: 6.325\nEpoch: 14, Train Loss: 6.207\nEpoch: 15, Train Loss: 6.199\nEpoch: 16, Train Loss: 6.170\nEpoch: 17, Train Loss: 6.367\nEpoch: 18, Train Loss: 6.145\nEpoch: 19, Train Loss: 6.220\nEpoch: 20, Train Loss: 6.229\nEpoch: 21, Train Loss: 6.201\nEpoch: 22, Train Loss: 6.274\nEpoch: 23, Train Loss: 6.358\nEpoch: 24, Train Loss: 6.086\nEpoch: 25, Train Loss: 6.204\nEpoch: 26, Train Loss: 6.155\nEpoch: 27, Train Loss: 6.425\nEpoch: 28, Train Loss: 6.327\nEpoch: 29, Train Loss: 6.311\nEpoch: 30, Train Loss: 5.984\nEpoch: 31, Train Loss: 6.325\nEpoch: 32, Train Loss: 6.163\nEpoch: 33, Train Loss: 6.284\nEpoch: 34, Train Loss: 6.282\nEpoch: 35, Train Loss: 6.173\nEpoch: 36, Train Loss: 6.049\nEpoch: 37, Train Loss: 6.153\nEpoch: 38, Train Loss: 6.228\nEpoch: 39, Train Loss: 6.147\nEpoch: 40, Train Loss: 6.265\nEpoch: 41, Train Loss: 6.346\nEpoch: 42, Train Loss: 6.138\nEpoch: 43, Train Loss: 6.128\nEpoch: 44, Train Loss: 6.128\nEpoch: 45, Train Loss: 6.118\nEpoch: 46, Train Loss: 6.214\nEpoch: 47, Train Loss: 6.127\nEpoch: 48, Train Loss: 6.368\nEpoch: 49, Train Loss: 6.065\nEpoch: 50, Train Loss: 6.077\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test accuracy  \nwith torch.no_grad():\n    correct = 0\n    for x, y in test_loader:\n        x = x.view(-1, 784)\n        y_pred = model(x).argmax(dim=1)\n        correct += (y_pred == y).sum().item() \nprint(f'Test accuracy: {correct/len(test_dataset):.5f}') ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:32:49.260418Z","iopub.execute_input":"2023-05-10T04:32:49.261349Z","iopub.status.idle":"2023-05-10T04:33:00.500020Z","shell.execute_reply.started":"2023-05-10T04:32:49.261314Z","shell.execute_reply":"2023-05-10T04:33:00.498845Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Test accuracy: 0.45643\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 3.2MCMC","metadata":{}},{"cell_type":"code","source":"# Get train data\ntrain_x = []  \ntrain_y = []\nfor x, y in train_loader:\n    train_x.append(x.view(-1, 784))\n    train_y.append(y)\ntrain_x = torch.cat(train_x)  \ntrain_y = torch.cat(train_y)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:33:00.501394Z","iopub.execute_input":"2023-05-10T04:33:00.501779Z","iopub.status.idle":"2023-05-10T04:33:10.835049Z","shell.execute_reply.started":"2023-05-10T04:33:00.501724Z","shell.execute_reply":"2023-05-10T04:33:10.833933Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Model with Uniform prior\nclass BNN(nn.Module):\n    def __init__(self, n_in, n_hide, n_out):\n        super().__init__()\n        self.fc1 = nn.Linear(n_in, n_hide)  \n        self.fc2 = nn.Linear(n_hide, n_out)\n        self.dropout = nn.Dropout(0.5)\n        self.w = torch.zeros(n_in, n_out)  # Weight parameter\n\n    def forward(self, x, w_proposal=None):\n        if w_proposal is None:\n              w = self.w\n        else:\n              w = w_proposal\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        pred = F.log_softmax(x, dim=1)\n        dist = torch.distributions.Categorical(logits=pred)\n        return dist\n\n    def log_prior(self, w):\n        return -torch.sum(torch.abs(w))","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:33:10.836562Z","iopub.execute_input":"2023-05-10T04:33:10.836906Z","iopub.status.idle":"2023-05-10T04:33:10.846516Z","shell.execute_reply.started":"2023-05-10T04:33:10.836877Z","shell.execute_reply":"2023-05-10T04:33:10.845281Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# MCMC Sampling\nmodel = BNN(784, 964 ,400) \nw = model.w.data  # Get weight tensor ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:33:10.847718Z","iopub.execute_input":"2023-05-10T04:33:10.848072Z","iopub.status.idle":"2023-05-10T04:33:10.871082Z","shell.execute_reply.started":"2023-05-10T04:33:10.848044Z","shell.execute_reply":"2023-05-10T04:33:10.869787Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"for i in range(1500): \n    w_proposal = torch.rand(w.size())\n\n    ap = torch.tensor(0.)  \n    \n    u = torch.rand(1)\n    if u < ap:  \n          w = w_proposal\n            \n    # update\n    model.w.data = w  \n\n    # loss\n    logits = model(train_x, w_proposal).logits\n    loss = F.cross_entropy(logits, train_y)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:37:16.300263Z","iopub.execute_input":"2023-05-10T04:37:16.300629Z","iopub.status.idle":"2023-05-10T04:37:20.809622Z","shell.execute_reply.started":"2023-05-10T04:37:16.300602Z","shell.execute_reply":"2023-05-10T04:37:20.808473Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Test accuracy\nwith torch.no_grad():\n    correct = 0\n    for x, y in test_loader:\n        x = x.view(-1, 784)\n        y_pred = model(x).sample()\n        correct += (y_pred == y).sum().item()\n    print(f'Test accuracy: {correct/len(test_dataset):.5f}') ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:39:21.284551Z","iopub.execute_input":"2023-05-10T04:39:21.284961Z","iopub.status.idle":"2023-05-10T04:39:32.524341Z","shell.execute_reply.started":"2023-05-10T04:39:21.284929Z","shell.execute_reply":"2023-05-10T04:39:32.523168Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Test accuracy: 0.09129\n","output_type":"stream"}]}]}